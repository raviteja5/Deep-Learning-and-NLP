{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gtnlplib.parsing as parsing\n",
    "import gtnlplib.constants as consts\n",
    "import gtnlplib.utils as utils\n",
    "import gtnlplib.feat_extractors as feat_extractors\n",
    "import gtnlplib.data_tools as data_tools\n",
    "import gtnlplib.neural_net as neural_net\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as ag\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset = data_tools.Dataset(consts.TRAIN_FILE, consts.DEV_FILE, consts.TEST_FILE)\n",
    "\n",
    "word_to_ix = { word: i for i, word in enumerate(dataset.vocab) }\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "#ETA_0 = 0.04 # SGD\n",
    "ETA_0=0.017 #Adam  \n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "\n",
    "LSTM_NUM_LAYERS = 2\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "STACK_EMBEDDING_DIM = 120\n",
    "NUM_FEATURES = 3\n",
    "\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "\n",
    "# BiLSTM over word embeddings\n",
    "word_embedding_lookup = neural_net.BiLSTMWordEmbeddingLookup(word_to_ix,\n",
    "                                                             WORD_EMBEDDING_DIM,\n",
    "                                                             STACK_EMBEDDING_DIM,\n",
    "                                                             num_layers=LSTM_NUM_LAYERS,\n",
    "                                                             dropout=DROPOUT)\n",
    "# Pretrained inputs\n",
    "utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
    "\n",
    "action_chooser = neural_net.ActionChooserNetwork(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "\n",
    "# LSTM reduction operations\n",
    "combiner = neural_net.LSTMCombinerNetwork(STACK_EMBEDDING_DIM,\n",
    "                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                          dropout=DROPOUT)\n",
    "\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                  action_chooser, combiner)\n",
    "\n",
    "    \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    optimizer = optim.Adam(parser.parameters(), lr=ETA_0)\n",
    "    print \"\\nValue:\",ETA_0\n",
    "    ETA_0 *= 0.5\n",
    "    print \"Epoch {}\".format(epoch+1)\n",
    "    parsing.train(dataset.training_data, bakeoff_parser, optimizer, verbose=True)\n",
    "\n",
    "    print \"Dev Evaluation\"\n",
    "    parsing.evaluate(dataset.dev_data, bakeoff_parser, verbose=True)\n",
    "    print \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
